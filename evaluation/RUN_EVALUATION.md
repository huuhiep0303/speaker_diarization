# ğŸš€ HÆ°á»›ng dáº«n cháº¡y Evaluation

## ğŸ“‹ Chuáº©n bá»‹

### 1. CÃ i Ä‘áº·t Dependencies

```bash
cd realtime/evaluation

# Core packages
pip install numpy scipy scikit-learn matplotlib tqdm

# Japanese text processing
pip install sudachipy

# ASR evaluation
pip install jiwer regex librosa soundfile

# Speaker diarization
pip install torch torchaudio speechbrain

# ASR models
pip install faster-whisper  # Whisper
pip install funasr         # SenseVoice
```

### 2. Kiá»ƒm tra Dataset

Dataset JVS Ä‘Ã£ cÃ³ sáºµn trong `realtime/dataset/jvs_ver1/`:

```
dataset/jvs_ver1/
â”œâ”€â”€ jvs001/
â”‚   â”œâ”€â”€ parallel100/
â”‚   â”‚   â”œâ”€â”€ transcripts_utf8.txt
â”‚   â”‚   â””â”€â”€ wav24kHz16bit/
â”‚   â”‚       â””â”€â”€ *.wav
â”‚   â”œâ”€â”€ nonpara30/
â”‚   â”œâ”€â”€ whisper10/
â”‚   â””â”€â”€ falset10/
â”œâ”€â”€ jvs002/
â””â”€â”€ ...
```

## ğŸ“Š BÆ°á»›c 1: Táº¡o Dataset CSV

Táº¡o file CSV chá»©a danh sÃ¡ch test cases tá»« JVS corpus:

```bash
# Táº¡o dataset vá»›i 1 sample/category/speaker (400 samples)
python create_dataset.py --jvs_root ../dataset/jvs_ver1 --output dataset_400_testcases.csv

# Hoáº·c táº¡o nhiá»u samples hÆ¡n
python create_dataset.py --jvs_root ../dataset/jvs_ver1 --samples_per_category 2 --output dataset_800_testcases.csv
```

**Output:** File CSV vá»›i format:

```csv
speaker,category,file_name,wav_path,transcript
jvs001,parallel100,VOICEACTRESS100_069,dataset/jvs_ver1/jvs001/parallel100/wav24kHz16bit/VOICEACTRESS100_069.wav,ãƒ–ãƒ«ãƒ¼ãƒªãƒƒã‚¸å±±è„ˆã®æºæµã‹ã‚‰...
```

## ğŸ¤ BÆ°á»›c 2: ÄÃ¡nh giÃ¡ ASR Quality

### Option A: ÄÃ¡nh giÃ¡ Whisper

```bash
# Whisper Small on CPU (khuyáº¿n nghá»‹ Ä‘á»ƒ test nhanh)
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --whisper_size small --device cpu --compute_type int8

# Whisper Small on GPU
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --whisper_size small --device cuda --compute_type float16

# Whisper Large-v3 on GPU (cháº¥t lÆ°á»£ng cao nháº¥t)
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --whisper_size large-v3 --device cuda --compute_type float16
```

### Option B: ÄÃ¡nh giÃ¡ SenseVoice

```bash
# SenseVoice on CPU
python eval_asr.py --dataset dataset_400_testcases.csv --model sensevoice --device cpu

# SenseVoice on GPU
python eval_asr.py --dataset dataset_400_testcases.csv --model sensevoice --device cuda
```

### Option C: Windows Batch Scripts

```cmd
REM Whisper
eval_asr.bat dataset_400_testcases.csv whisper cpu

REM SenseVoice
eval_asr.bat dataset_400_testcases.csv sensevoice cpu
```

### â¸ï¸ Resume tá»« Checkpoint

Náº¿u quÃ¡ trÃ¬nh bá»‹ giÃ¡n Ä‘oáº¡n, chá»‰ cáº§n cháº¡y láº¡i lá»‡nh tÆ°Æ¡ng tá»±:

```bash
# Script tá»± Ä‘á»™ng resume tá»« checkpoint
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --resume
```

### ğŸ“ˆ Káº¿t quáº£ ASR

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong `eval_results/`:

**Checkpoint CSV** (`eval_whisper-small_checkpoint.csv`):

```csv
file_path,ground_truth,prediction,wer,cer,rtf,audio_duration,processing_time
dataset/.../audio1.wav,ãƒ–ãƒ«ãƒ¼ãƒªãƒƒã‚¸å±±è„ˆ...,ãƒ–ãƒ«ãƒ¼ãƒªãƒƒã‚¸å±±è„ˆ...,0.0523,0.0234,0.45,5.2,2.34
```

**Summary JSON** (`eval_whisper-small_summary.json`):

```json
{
  "model": "whisper-small",
  "num_samples": 400,
  "avg_wer": 0.1234,
  "avg_cer": 0.0567,
  "avg_rtf": 0.45,
  "median_rtf": 0.42
}
```

**Metrics giáº£i thÃ­ch:**

- **WER** (Word Error Rate): Tá»· lá»‡ lá»—i tá»« - cÃ ng tháº¥p cÃ ng tá»‘t
  - < 10%: Excellent
  - 10-20%: Good
  - 20-30%: Fair
  - \> 30%: Poor
- **CER** (Character Error Rate): Tá»· lá»‡ lá»—i kÃ½ tá»± - cho tiáº¿ng Nháº­t

  - < 5%: Excellent
  - 5-10%: Good
  - 10-15%: Fair
  - \> 15%: Poor

- **RTF** (Real-Time Factor): Tá»‘c Ä‘á»™ xá»­ lÃ½
  - < 1.0: Nhanh hÆ¡n realtime âœ“ (vÃ­ dá»¥: 0.5 = 2x nhanh hÆ¡n)
  - = 1.0: ÄÃºng realtime
  - \> 1.0: Cháº­m hÆ¡n realtime âœ—

## ğŸ‘¥ BÆ°á»›c 3: ÄÃ¡nh giÃ¡ Speaker Diarization

### Cháº¡y Evaluation

```bash
# ÄÃ¡nh giÃ¡ vá»›i SpeechBrain ECAPA-TDNN
python eval_diarization.py --data_dir ../dataset/jvs_ver1 --model speechbrain --max_genuine 50 --max_impostor 100 --use_cache

# Windows
eval_diarization.bat ..\dataset\jvs_ver1 speechbrain
```

**Tham sá»‘:**

- `--data_dir`: ThÆ° má»¥c JVS (sáº½ quÃ©t táº¥t cáº£ speakers)
- `--model`: speechbrain (hiá»‡n táº¡i chá»‰ há»— trá»£ SpeechBrain)
- `--max_genuine`: Sá»‘ cáº·p genuine pairs/speaker (cÃ¹ng ngÆ°á»i)
- `--max_impostor`: Sá»‘ cáº·p impostor pairs/speaker (khÃ¡c ngÆ°á»i)
- `--use_cache`: LÆ°u embeddings cache Ä‘á»ƒ cháº¡y láº¡i nhanh hÆ¡n

### ğŸ“ˆ Káº¿t quáº£ Diarization

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u trong `eval_results/`:

**JSON** (`eval_results_speechbrain.json`):

```json
{
  "eer": 0.0523,
  "threshold_at_eer": 0.6234,
  "far_at_eer": 0.0523,
  "frr_at_eer": 0.0523,
  "best_f1": 0.9512,
  "roc_auc": 0.9876,
  "pr_auc": 0.9823
}
```

**Plots:**

- `eval_results_speechbrain_curves.png`: ROC vÃ  PR curves

**Metrics giáº£i thÃ­ch:**

- **EER** (Equal Error Rate): Tá»· lá»‡ lá»—i khi FAR = FRR
  - < 1%: Excellent
  - 1-5%: Good
  - 5-10%: Fair
  - \> 10%: Poor
- **FAR** (False Accept Rate): Cháº¥p nháº­n nháº§m ngÆ°á»i khÃ¡c
- **FRR** (False Reject Rate): Tá»« chá»‘i nháº§m cÃ¹ng ngÆ°á»i
- **ROC AUC**: Diá»‡n tÃ­ch dÆ°á»›i ROC curve (gáº§n 1.0 = tá»‘t)
- **Best F1**: F1-score tá»‘t nháº¥t (balance giá»¯a precision vÃ  recall)

## ğŸ”„ So sÃ¡nh nhiá»u Models

### ASR Comparison

```bash
# Whisper Small
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --whisper_size small --device cpu

# Whisper Medium
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --whisper_size medium --device cpu

# SenseVoice
python eval_asr.py --dataset dataset_400_testcases.csv --model sensevoice --device cpu

# So sÃ¡nh káº¿t quáº£
ls eval_results/eval_*_summary.json
```

### Compare Results

Má»Ÿ cÃ¡c file JSON trong `eval_results/` vÃ  so sÃ¡nh:

| Model          | WER   | CER   | RTF  |
| -------------- | ----- | ----- | ---- |
| whisper-small  | 0.123 | 0.056 | 0.45 |
| whisper-medium | 0.098 | 0.043 | 0.78 |
| sensevoice     | 0.145 | 0.068 | 0.32 |

## ğŸ› ï¸ Troubleshooting

### Lá»—i thiáº¿u packages

```bash
pip install sudachipy jiwer regex librosa soundfile
pip install faster-whisper funasr
pip install torch torchaudio speechbrain
```

### Lá»—i CUDA out of memory

```bash
# DÃ¹ng CPU hoáº·c model nhá» hÆ¡n
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --whisper_size tiny --device cpu
```

### Lá»—i khÃ´ng tÃ¬m tháº¥y audio files

```bash
# Kiá»ƒm tra Ä‘Æ°á»ng dáº«n trong CSV
head dataset_400_testcases.csv

# Äáº£m báº£o Ä‘Æ°á»ng dáº«n tÆ°Æ¡ng Ä‘á»‘i Ä‘Ãºng
cd realtime/evaluation
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper
```

### Lá»—i Japanese tokenizer

```bash
# CÃ i Ä‘áº·t Sudachi dictionary
pip install sudachipy
python -m sudachipy link -t full
```

### XÃ³a cache vÃ  cháº¡y láº¡i

```bash
# XÃ³a cache embeddings
rm -rf eval_cache/*

# XÃ³a checkpoint Ä‘á»ƒ cháº¡y láº¡i tá»« Ä‘áº§u
rm eval_results/eval_*_checkpoint.csv
```

## ğŸ“Š Expected Performance

### JVS Dataset (400 samples)

**Whisper Small (CPU, int8):**

- WER: ~8-12%
- CER: ~4-6%
- RTF: ~0.3-0.5
- Time: ~30-40 minutes

**Whisper Large-v3 (GPU, float16):**

- WER: ~5-8%
- CER: ~2-4%
- RTF: ~0.6-0.8
- Time: ~45-60 minutes

**SenseVoice (CPU):**

- WER: ~10-15%
- CER: ~5-8%
- RTF: ~0.2-0.3
- Time: ~20-30 minutes

**SpeechBrain Diarization:**

- EER: ~2-5%
- Time: ~20-30 minutes (with cache)

## ğŸ’¡ Tips

1. **Báº¯t Ä‘áº§u vá»›i dataset nhá»** Ä‘á»ƒ test:

   ```bash
   # Chá»‰ 100 samples
   python create_dataset.py --jvs_root ../dataset/jvs_ver1 --samples_per_category 0.25 --output dataset_100_test.csv
   ```

2. **DÃ¹ng CPU cho test nhanh**, GPU cho production
3. **Cache embeddings** giÃºp evaluation nhanh hÆ¡n nhiá»u
4. **Checkpoint tá»± Ä‘á»™ng** - cÃ³ thá»ƒ dá»«ng vÃ  resume báº¥t cá»© lÃºc nÃ o
5. **So sÃ¡nh nhiá»u models** Ä‘á»ƒ chá»n model tá»‘t nháº¥t cho use case

## ğŸ“ Next Steps

Sau khi cÃ³ káº¿t quáº£ evaluation:

1. **Chá»n model tá»‘t nháº¥t** dá»±a trÃªn WER/CER vÃ  RTF
2. **Fine-tune parameters** náº¿u cáº§n (beam size, temperature, etc.)
3. **Test vá»›i real-world audio** tá»« á»©ng dá»¥ng cá»§a báº¡n
4. **Deploy model** vÃ o production

---

**Happy Evaluating! ğŸ‰**

Náº¿u cáº§n há»— trá»£, check file `README.md` hoáº·c `QUICKSTART.md` trong folder nÃ y.
