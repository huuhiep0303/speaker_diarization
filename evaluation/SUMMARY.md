# ğŸ“Š Há»‡ thá»‘ng Evaluation - Tá»•ng quan

## âœ… ÄÃ£ hoÃ n thÃ nh

ÄÃ£ táº¡o há»‡ thá»‘ng evaluation hoÃ n chá»‰nh cho cÃ¡c models trong folder `realtime/`:

### 1. **create_dataset.py** - Táº¡o dataset tá»« JVS Corpus

- âœ… Há»— trá»£ JVS dataset structure
- âœ… Random sampling tá»« 4 categories (parallel100, nonpara30, whisper10, falset10)
- âœ… Export CSV vá»›i format chuáº©n
- âœ… Thá»‘ng kÃª dataset

### 2. **eval_asr.py** - ÄÃ¡nh giÃ¡ ASR Quality

- âœ… Há»— trá»£ Whisper (faster-whisper)
- âœ… Há»— trá»£ SenseVoice (FunASR)
- âœ… Japanese tokenization vá»›i Sudachi (WER tÃ­nh theo tá»«)
- âœ… Metrics: WER, CER, RTF
- âœ… Checkpoint mechanism (auto-resume)
- âœ… Output: CSV chi tiáº¿t + JSON summary

### 3. **eval_diarization.py** - ÄÃ¡nh giÃ¡ Speaker Verification

- âœ… Há»— trá»£ SpeechBrain ECAPA-TDNN
- âœ… Genuine/Impostor trials
- âœ… Metrics: EER, FAR, FRR, F1, AUC
- âœ… Embedding cache
- âœ… ROC & PR curves visualization

### 4. **Documentation**

- âœ… README.md - HÆ°á»›ng dáº«n tá»•ng quan
- âœ… RUN_EVALUATION.md - HÆ°á»›ng dáº«n cháº¡y chi tiáº¿t
- âœ… QUICKSTART_EVALUATION.md - Quick start 3 bÆ°á»›c
- âœ… Batch scripts cho Windows

### 5. **Folder Structure**

```
evaluation/
â”œâ”€â”€ create_dataset.py
â”œâ”€â”€ eval_asr.py
â”œâ”€â”€ eval_diarization.py
â”œâ”€â”€ eval_asr.bat
â”œâ”€â”€ eval_diarization.bat
â”œâ”€â”€ setup.bat
â”œâ”€â”€ README.md
â”œâ”€â”€ RUN_EVALUATION.md
â”œâ”€â”€ QUICKSTART_EVALUATION.md
â”œâ”€â”€ eval_cache/          # Cache embeddings
â”œâ”€â”€ eval_results/        # Káº¿t quáº£ evaluation
â””â”€â”€ test_audio/         # Audio files (optional)
```

---

## ğŸ¯ Models Ä‘Æ°á»£c Ä‘Ã¡nh giÃ¡

### ASR Models

1. **Whisper** (faster-whisper)

   - Sizes: tiny, base, small, medium, large, large-v3, turbo
   - Device: CPU, CUDA
   - Compute type: int8, float16, float32

2. **SenseVoice** (FunASR)
   - Model: FunAudioLLM/SenseVoiceSmall
   - Device: CPU, CUDA
   - Optimized cho tiáº¿ng Nháº­t

### Diarization Models

1. **SpeechBrain ECAPA-TDNN**
   - Model: speechbrain/spkrec-ecapa-voxceleb
   - Speaker embedding extraction
   - Cosine similarity for verification

---

## ğŸ“‹ CÃ¡ch sá»­ dá»¥ng

### Quick Start (3 bÆ°á»›c)

```bash
# 1. Táº¡o dataset
python create_dataset.py --jvs_root ../dataset/jvs_ver1

# 2. Cháº¡y ASR evaluation
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --device cpu

# 3. Xem káº¿t quáº£
cat eval_results/eval_whisper-small_summary.json
```

### Windows Users

```cmd
REM ASR Evaluation
eval_asr.bat dataset_400_testcases.csv whisper cpu

REM Diarization Evaluation
eval_diarization.bat ..\dataset\jvs_ver1 speechbrain
```

---

## ğŸ“Š Output Files

### ASR Evaluation

```
eval_results/
â”œâ”€â”€ eval_whisper-small_checkpoint.csv    # Chi tiáº¿t tá»«ng sample
â””â”€â”€ eval_whisper-small_summary.json      # Thá»‘ng kÃª tá»•ng há»£p
```

**Checkpoint CSV format:**

```csv
file_path,ground_truth,prediction,wer,cer,rtf,audio_duration,processing_time
```

**Summary JSON format:**

```json
{
  "model": "whisper-small",
  "num_samples": 400,
  "avg_wer": 0.1234,
  "avg_cer": 0.0567,
  "avg_rtf": 0.45,
  "median_rtf": 0.42
}
```

### Diarization Evaluation

```
eval_results/
â”œâ”€â”€ eval_results_speechbrain.json        # Metrics
â””â”€â”€ eval_results_speechbrain_curves.png  # ROC & PR plots
```

**Metrics JSON format:**

```json
{
  "eer": 0.0523,
  "threshold_at_eer": 0.6234,
  "far_at_eer": 0.0523,
  "frr_at_eer": 0.0523,
  "best_f1": 0.9512,
  "roc_auc": 0.9876
}
```

---

## ğŸ”§ TÃ­nh nÄƒng chÃ­nh

### 1. Checkpoint Auto-Resume

- âœ… Tá»± Ä‘á»™ng lÆ°u progress sau má»—i sample
- âœ… CÃ³ thá»ƒ dá»«ng vÃ  resume báº¥t cá»© lÃºc nÃ o
- âœ… KhÃ´ng máº¥t cÃ´ng viá»‡c Ä‘Ã£ lÃ m

### 2. Japanese Text Processing

- âœ… Sudachi tokenizer cho WER calculation
- âœ… Text normalization cho tiáº¿ng Nháº­t
- âœ… Remove punctuation, tags, spaces

### 3. Embedding Cache

- âœ… Cache speaker embeddings
- âœ… Cháº¡y láº¡i nhanh hÆ¡n nhiá»u láº§n
- âœ… Tá»± Ä‘á»™ng detect cache

### 4. Comprehensive Metrics

**ASR:**

- WER (Word Error Rate) - vá»›i Japanese tokenization
- CER (Character Error Rate)
- RTF (Real-Time Factor)

**Diarization:**

- EER (Equal Error Rate)
- FAR/FRR (False Accept/Reject Rate)
- F1, Precision, Recall
- ROC AUC, PR AUC

---

## ğŸ“ˆ Expected Results

### JVS Dataset (400 samples)

**Whisper Small (CPU):**

- WER: ~8-12%
- CER: ~4-6%
- RTF: ~0.3-0.5 (2x faster than realtime)
- Time: ~30-40 minutes

**SenseVoice (CPU):**

- WER: ~10-15%
- CER: ~5-8%
- RTF: ~0.2-0.3 (3x faster than realtime)
- Time: ~20-30 minutes

**SpeechBrain Diarization:**

- EER: ~2-5%
- ROC AUC: ~0.97-0.99
- Time: ~20-30 minutes (with cache)

---

## ğŸš€ Next Steps

### 1. Cháº¡y Evaluation

```bash
cd realtime/evaluation

# Táº¡o dataset
python create_dataset.py --jvs_root ../dataset/jvs_ver1

# ÄÃ¡nh giÃ¡ Whisper
python eval_asr.py --dataset dataset_400_testcases.csv --model whisper --device cpu

# ÄÃ¡nh giÃ¡ SenseVoice
python eval_asr.py --dataset dataset_400_testcases.csv --model sensevoice --device cpu

# ÄÃ¡nh giÃ¡ Diarization
python eval_diarization.py --data_dir ../dataset/jvs_ver1 --model speechbrain --use_cache
```

### 2. So sÃ¡nh káº¿t quáº£

```bash
# Xem táº¥t cáº£ summary files
ls eval_results/*_summary.json

# So sÃ¡nh metrics
cat eval_results/eval_whisper-small_summary.json
cat eval_results/eval_sensevoice_summary.json
```

### 3. Chá»n model tá»‘t nháº¥t

- **Whisper Small**: Balance tá»‘t (WER ~10%, RTF ~0.4)
- **Whisper Large-v3**: Cháº¥t lÆ°á»£ng cao nháº¥t (WER ~6%, RTF ~0.8)
- **SenseVoice**: Nhanh nháº¥t (WER ~12%, RTF ~0.3)

### 4. Deploy vÃ o production

- Integrate model Ä‘Æ°á»£c chá»n vÃ o `realtime_diarization_improved.py`
- Test vá»›i real-world audio
- Monitor performance

---

## ğŸ“š Documentation

**Äá»c thÃªm:**

- `README.md` - Tá»•ng quan há»‡ thá»‘ng
- `RUN_EVALUATION.md` - HÆ°á»›ng dáº«n chi tiáº¿t tá»«ng bÆ°á»›c
- `QUICKSTART_EVALUATION.md` - Quick start 3 bÆ°á»›c
- `test_audio/README.md` - HÆ°á»›ng dáº«n chuáº©n bá»‹ audio

**Troubleshooting:**

- Check `RUN_EVALUATION.md` section "Troubleshooting"
- Xem error messages trong terminal
- Kiá»ƒm tra checkpoint files trong `eval_results/`

---

## âœ¨ Features

- âœ… **Easy to use** - Chá»‰ cáº§n 3 lá»‡nh
- âœ… **Auto-resume** - Checkpoint mechanism
- âœ… **Japanese support** - Sudachi tokenizer
- âœ… **Comprehensive metrics** - WER, CER, RTF, EER, AUC
- âœ… **Visualization** - ROC & PR curves
- âœ… **Cache embeddings** - Fast re-evaluation
- âœ… **Windows support** - Batch scripts
- âœ… **Well documented** - Multiple README files

---

## ğŸ‰ Summary

Há»‡ thá»‘ng evaluation hoÃ n chá»‰nh Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ:

1. âœ… ÄÃ¡nh giÃ¡ ASR quality (Whisper, SenseVoice)
2. âœ… ÄÃ¡nh giÃ¡ Speaker Diarization (SpeechBrain)
3. âœ… So sÃ¡nh nhiá»u models
4. âœ… Chá»n model tá»‘t nháº¥t cho production

**Báº¯t Ä‘áº§u ngay:** Xem `QUICKSTART_EVALUATION.md`

**Good luck with your evaluation! ğŸš€**

---

**Created:** November 28, 2025  
**Author:** VJ Speaker Diarization Team  
**Version:** 1.0
